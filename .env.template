# octo-cl Configuration Template
# Copy this to .env and modify as needed

# The URL where your Ollama server is running
OLLAMA_URL=http://localhost:11434

# The model to use by default (e.g., qwen2.5-coder:7b, llama3.1, etc.)
OCTO_MODEL=qwen2.5-coder:7b
